{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32781a12",
   "metadata": {},
   "source": [
    "# Analysis on Final Dataset\n",
    "\n",
    "The main problem is that I have to go through multiple files and if each file naturally causes such high degree networks, it is good to initially ignore these and see if the static cut-off approach is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f74a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd1de2",
   "metadata": {},
   "source": [
    "Of course, there is always a slight change in the code which in the end makes it look repetative. Trying to avoid that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueAuthors():\n",
    "    \"\"\"\n",
    "    Get Unique Authors for given paper\n",
    "    INPUT:\n",
    "        json: contains publications information\n",
    "    OUTPUT:\n",
    "        temp_mem: contains unique authors from one batch.  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    temp_mem = []\n",
    "    r = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "    count = 0\n",
    "    for i in range(len(data['content'])):\n",
    "        for j in range(len(data['content'][i]['authorships'])):\n",
    "            if (data['content'][i]['authorships'][j]['otype'] == 'PersonAuthorship'):\n",
    "                string_to_add = '' \n",
    "                if 'givenName' in data['content'][i]['authorships'][j]:\n",
    "                    string_to_add += str(data['content'][i]['authorships'][j]['givenName']) + ' '\n",
    "                if 'familyName' in data['content'][i]['authorships'][j]:\n",
    "                    string_to_add += str(data['content'][i]['authorships'][j]['familyName'])\n",
    "                temp_mem.append(r.sub(\"\",string_to_add))\n",
    "\n",
    "\n",
    "    temp_mem = list(np.unique(temp_mem))\n",
    "    #temp_mem = dict(zip(temp_mem, np.array(np.arange(len(temp_mem)),dtype=np.uint32 )) ) \n",
    "    return temp_mem\n",
    "\n",
    "def getFileAuthorLinks(temp_link_list, encoder, limit):\n",
    "    \"\"\"\n",
    "    Gets links beteen Unique authors to \n",
    "    \n",
    "    INPUT:\n",
    "        temp_link_list: contains edges\n",
    "        encoder: list with encoded unique authors\n",
    "        limit: limit to ignore pubications by comparing to their authourCount\n",
    "    \n",
    "    OUTPUT:\n",
    "        VOID: modifies temp_link_ist accordingly\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def getAuthorLinks(encoder, limit=20):\n",
    "    _name_list = np.array(list(encoder.keys()))      #saving tables\n",
    "    _encoded_list = np.array(list(encoder.values())) \n",
    "    temp_link_list = np.zeros((0,4), dtype=np.uint32) #burning in datatype\n",
    "    r = re.compile(r\"\\s+\", re.MULTILINE)              #for stripping some stuff\n",
    "\n",
    "    for i in range(len(data['content'])):\n",
    "        if (i+1) % 10 == 0:\n",
    "            sys.stdout.write(\"\\r {0} / {1} # {2}\".format(i+1,len(data['content']), len(temp_link_list)))\n",
    "            sys.stdout.flush()\n",
    "        content_mem = []\n",
    "        for j in range(len(data['content'][i]['authorships'])):\n",
    "\n",
    "            if (data['content'][i]['authorships'][j]['otype'] == 'PersonAuthorship'):\n",
    "                name_to_add = ''\n",
    "                string_to_add = '' \n",
    "                #if the field extists, get the name! -> what if none of them exists?\n",
    "                if 'givenName' in data['content'][i]['authorships'][j]:\n",
    "                    name_to_add += str(data['content'][i]['authorships'][j]['givenName']) + ' '\n",
    "                if 'familyName' in data['content'][i]['authorships'][j]:\n",
    "                    name_to_add += str(data['content'][i]['authorships'][j]['familyName'])\n",
    "                name_to_add = r.sub(\"\", name_to_add)\n",
    "                _encoded = _encoded_list[_name_list == name_to_add]\n",
    "                if len(_encoded) == 1:\n",
    "                    content_mem.append(_encoded[0])\n",
    "                else:\n",
    "                    print(\"Wrong amount of authors: {} != 1 !\".format(len(_encoded)))\n",
    "\n",
    "        if (len(content_mem) > 0):\n",
    "            tmp_mem = []     #save partial results as constans addition is not liked by numpy\n",
    "            for j in range(len(content_mem)):\n",
    "                for k in np.arange(j+1,len(content_mem)):\n",
    "                    tmp_mem.append([content_mem[j],content_mem[k],data['content'][i]['publishedYear'],data['content'][i]['authorCount']])\n",
    "            if (len(tmp_mem) > 0): \n",
    "                temp_link_list = np.append(temp_link_list,\n",
    "                                           tmp_mem,\n",
    "                                           axis=0)\n",
    "\n",
    "    temp_link_list = np.array(temp_link_list, dtype=np.uint32)\n",
    "    \n",
    "    return temp_link_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
