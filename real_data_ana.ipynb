{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32781a12",
   "metadata": {},
   "source": [
    "# Analysis on Final Dataset\n",
    "\n",
    "The main problem is that I have to go through multiple files and if each file naturally causes such high degree networks, it is good to initially ignore these and see if the static cut-off approach is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f74a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd1de2",
   "metadata": {},
   "source": [
    "Of course, there is always a slight change in the code which in the end makes it look repetative. Trying to avoid that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9fcc52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueAuthors(data, limit=20):\n",
    "    \"\"\"\n",
    "    Get Unique Authors for given paper\n",
    "    Needs to be done after each batch and another numpy.unique after that\n",
    "    \n",
    "    INPUT:\n",
    "        data (json): contains publications information\n",
    "    OUTPUT:\n",
    "        temp_mem: contains unique authors from one batch.  \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    temp_mem = []\n",
    "    r = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "    count = 0\n",
    "    for i in range(len(data['content'])):\n",
    "        if ('authorships' in data['content'][i].keys() ) and (limit >= len(data['content'][i]['authorships'])):       #\"yes, someone forgets to fill this\"\n",
    "            for j in range(len(data['content'][i]['authorships'])):\n",
    "                if (data['content'][i]['authorships'][j]['otype'] == 'PersonAuthorship'):\n",
    "                    string_to_add = '' \n",
    "                    if 'givenName' in data['content'][i]['authorships'][j]:\n",
    "                        string_to_add += str(data['content'][i]['authorships'][j]['givenName']) + ' '\n",
    "                    if 'familyName' in data['content'][i]['authorships'][j]:\n",
    "                        string_to_add += str(data['content'][i]['authorships'][j]['familyName'])\n",
    "                    if len(string_to_add) > 0:\n",
    "                        temp_mem.append(r.sub(\"\",string_to_add))\n",
    "\n",
    "\n",
    "    temp_mem = np.array(list(np.unique(temp_mem)), dtype=object)   # define it as an object\n",
    "    #temp_mem = dict(zip(temp_mem, np.array(np.arange(len(temp_mem)),dtype=np.uint32 )) ) \n",
    "    return temp_mem\n",
    "\n",
    "\n",
    "def getAuthorLinks(data,temp_link_list,encoder, limit=20): \n",
    "    _name_list = np.array(list(encoder.keys()))            #saving tables\n",
    "    _encoded_list = np.array(list(encoder.values())) \n",
    "    \n",
    "    temp_link_list = np.zeros((0,4), dtype=np.uint32)\n",
    "    r = re.compile(r\"\\s+\", re.MULTILINE)                   #for stripping some stuff\n",
    "\n",
    "    for i in range(len(data['content'])):\n",
    "        if (i+1) % 10 == 0:\n",
    "            sys.stdout.write(\"\\r {0} / {1} # {2}\".format(i+1,len(data['content']), len(temp_link_list)))\n",
    "            sys.stdout.flush()\n",
    "        content_mem = []\n",
    "        if ('authorships' in data['content'][i].keys()) and (limit >= len(data['content'][i]['authorships'])):       #\"yes, someone forgets to fill this\"\n",
    "            for j in range(len(data['content'][i]['authorships'])):\n",
    "\n",
    "                if (data['content'][i]['authorships'][j]['otype'] == 'PersonAuthorship'):\n",
    "                    name_to_add = ''\n",
    "                    string_to_add = '' \n",
    "                    #if the field extists, get the name! -> what if none of them exists? don't give anything\n",
    "                    if 'givenName' in data['content'][i]['authorships'][j]:\n",
    "                        name_to_add += str(data['content'][i]['authorships'][j]['givenName']) + ' '\n",
    "                    if 'familyName' in data['content'][i]['authorships'][j]:\n",
    "                        name_to_add += str(data['content'][i]['authorships'][j]['familyName'])\n",
    "                    name_to_add = r.sub(\"\", name_to_add)\n",
    "                    _encoded = _encoded_list[_name_list == name_to_add]\n",
    "                    if len(_encoded) == 1:\n",
    "                        content_mem.append(_encoded[0])\n",
    "                    else:\n",
    "                        print(\"Wrong amount of authors: {} != 1 !\".format(len(_encoded)))\n",
    "\n",
    "        if (len(content_mem) > 0):\n",
    "            tmp_mem = []     #save partial results as constans addition is not liked by numpy\n",
    "            for j in range(len(content_mem)):\n",
    "                for k in np.arange(j+1,len(content_mem)):\n",
    "                    tmp_mem.append([content_mem[j],content_mem[k],data['content'][i]['publishedYear'],data['content'][i]['authorCount']])\n",
    "            if (len(tmp_mem) > 0): \n",
    "                temp_link_list = np.append(temp_link_list,\n",
    "                                           tmp_mem,\n",
    "                                           axis=0)\n",
    "\n",
    "    temp_link_list = np.array(temp_link_list, dtype=np.uint32)\n",
    "    return temp_link_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2f24b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesUniqueAuthors(loc, limit=20):\n",
    "    \"\"\"\n",
    "    Get Unique Authors for all batches, uses getUniqueAuthors\n",
    "    \n",
    "    INPUT:\n",
    "        loc: path to the batches' folder\n",
    "    OUTPUT:\n",
    "        uniqueAuthorsEncoded: unique authors from all batches\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    uniqueAuthors = np.array([], dtype=object)   #initiate the container \n",
    "    files = os.listdir(loc)                      #get files in the given folder\n",
    "    \n",
    "    assert len(files) >= 1, \"No files found! 1 >= {}\".format(len(files))\n",
    "    for i in range(len(files)):\n",
    "        sub_path = loc + \"/\" + files[i]\n",
    "        print(sub_path)\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        #technically os.listdir lists everything in the folder, not only files \n",
    "        #i have to try to open them\n",
    "        sub_path = loc + \"/\" + files[i]\n",
    "        \n",
    "        try:       \n",
    "            file = open(sub_path,\"rt\",encoding=\"utf-8\") #this is the correct way to open\n",
    "            data = json.load(file)                                      #use json library to open\n",
    "            file.close()                                                #close immidiately\n",
    "        except:\n",
    "            print(\"Cannot open {}\".format(sub_path))\n",
    "            pass\n",
    "        \n",
    "        #proceed \n",
    "        batchUniqueAuthors = getUniqueAuthors(data, limit=limit)\n",
    "        uniqueAuthors = np.append( uniqueAuthors, batchUniqueAuthors )\n",
    "\n",
    "        sys.stdout.write(\"\\rFILE: {}\\t{}/{} -> {} / {}\\t\\t\\t\\t\".format(sub_path,\n",
    "                                                               i+1,\n",
    "                                                               len(files),\n",
    "                                                               len(batchUniqueAuthors),\n",
    "                                                               len(uniqueAuthors)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    \n",
    "            \n",
    "    \n",
    "    #when finished, use np.unqiue again\n",
    "    bashedAmount = len(uniqueAuthors)\n",
    "    \n",
    "    uniqueAuthors = np.unique(uniqueAuthors)\n",
    "    uniqueAuthorsEncoded = dict(zip(uniqueAuthors, np.array(np.arange(len(uniqueAuthors)), dtype=np.uint32)))\n",
    "    \n",
    "    print(\"{} to {} actually exists\".format(bashedAmount,len(uniqueAuthors)))\n",
    "    return uniqueAuthorsEncoded\n",
    "\n",
    "def getFilesAuthorLinks(loc, encoder, limit=20):\n",
    "    \"\"\"\n",
    "    Gets links beteen Unique authors to \n",
    "    \n",
    "    INPUT:\n",
    "        temp_link_list: contains edges\n",
    "        encoder: list with encoded unique authors\n",
    "        limit: limit to ignore pubications by comparing to their authourCount\n",
    "    \n",
    "    OUTPUT:\n",
    "        VOID: modifies temp_link_ist accordingly\n",
    "    \n",
    "    \"\"\"\n",
    "    tmp_lnk_lst = np.zeros((0,4), dtype=np.uint32)      #burning in datatype\n",
    "    files = os.listdir(loc)\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        #technically os.listdir lists everything in the folder, not only files \n",
    "        #i have to try to open them\n",
    "        sub_path = loc + \"/\" + files[i]\n",
    "        \n",
    "        try:       \n",
    "            file = open(sub_path,\"rt\",encoding=\"utf-8\") #this is the correct way to open\n",
    "            data = json.load(file)                                      #use json library to open\n",
    "            file.close()                                                #close immidiately\n",
    "        except:\n",
    "            print(\"Cannot open {}\".format(sub_path))\n",
    "            pass\n",
    "        \n",
    "        #proceed \n",
    "        batchAuthorLinks = getAuthorLinks(data=data, temp_link_list=tmp_lnk_lst, encoder=encoder, limit=limit)\n",
    "        tmp_lnk_lst = np.append( tmp_lnk_lst, batchAuthorLinks, axis=0 )\n",
    "        sys.stdout.write(\"\\rFILE: {}\\t{}/{} -> {}/{}\\t\\t\\t\\t\".format(sub_path,\n",
    "                                                               i+1,\n",
    "                                                               len(files),\n",
    "                                                               len(batchAuthorLinks),\n",
    "                                                               len(tmp_lnk_lst)\n",
    "                                                               ))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    return tmp_lnk_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5aa18fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final/publications_1.json\n",
      "data/final/publications_10.json\n",
      "data/final/publications_11.json\n",
      "data/final/publications_12.json\n",
      "data/final/publications_13.json\n",
      "data/final/publications_14.json\n",
      "data/final/publications_2.json\n",
      "data/final/publications_3.json\n",
      "data/final/publications_4.json\n",
      "data/final/publications_5.json\n",
      "data/final/publications_6.json\n",
      "data/final/publications_7.json\n",
      "data/final/publications_8.json\n",
      "data/final/publications_9.json\n",
      "FILE: data/final/publications_9.json\t14/14 -> 7111 / 95824\t\t\t\t95824 to 51450 actually exists\n"
     ]
    }
   ],
   "source": [
    "path = \"data/final\"\n",
    "\n",
    "_UniqueAuthors = getFilesUniqueAuthors(loc=path, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ab4241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: data/final/publications_9.json\t14/14 -> 25349/1353480\t\t\t\t"
     ]
    }
   ],
   "source": [
    "tmp_lnk_lst = getFilesAuthorLinks(loc=path, encoder = _UniqueAuthors, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a9c2eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22082, 13837,  2014, ..., 23669,  2019,     7], dtype=uint32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_lnk_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9cb1ff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  1,  1, -2, -2, -1, -1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array([[2,2],[1,1]]), np.array([[-2,-2],[-1,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
